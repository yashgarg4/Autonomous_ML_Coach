# Autonomous ML Coach

This project is an autonomous multi-agent workflow designed to take a user prompt, research the topic, write Python code, generate tests, and iteratively debug the code until it meets the requirements.

## Table of Contents

- Overview
- Features
- Workflow
- Project Structure
- Setup and Installation
- Running the Workflow
- Configuration

## Overview

The "Autonomous ML Coach" automates the initial phases of software development. It starts with a high-level prompt and orchestrates a series of AI agents to produce a functional and tested Python script. The system is designed to be resilient, with built-in retries, code sanitization, and a debugging loop that can automatically apply fixes.

## Features

- **Multi-Agent System**: Utilizes distinct agents for different tasks:
  - **Researcher**: Gathers information based on the user prompt.
  - **Coder**: Writes Python code based on the researcher's specification.
  - **Test-Writer**: Generates `pytest` tests for the generated code.
  - **Debugger**: Analyzes syntax errors, runtime errors, and test failures to propose patches.
- **Autonomous Debugging Loop**: The workflow can run for multiple iterations. In each cycle, the debugger analyzes the output and can suggest a `PATCH` to fix issues.
- **Auto-Correction**: The system can automatically apply patches suggested by the debugger, especially for initial syntax errors. It can also be configured to apply all patches automatically or prompt the user for confirmation.
- **Code Sanitization**: Includes robust logic to clean and sanitize the raw output from the language models to ensure it is valid and runnable Python code.
- **Built-in Testing**: Automatically runs static syntax checks and executes the generated `pytest` suite to validate the code's correctness.

## Workflow

The process for each iteration is as follows:

1.  **Research**: The `researcher` agent processes the initial user prompt to create a detailed specification.
2.  **Code Generation**: The `coder` agent takes the specification and writes the Python code. This step includes a retry mechanism to ensure the output is valid Python. The final code is saved to `generated_code.py`.
3.  **Inspection**: The generated code is checked for syntax errors and is executed to catch any immediate runtime issues.
4.  **Test Generation**: The `test-writer` agent creates a suite of `pytest` tests based on the generated code, which are saved to `test_generated.py`.
5.  **Pytest Execution**: The test suite is run against the generated code.
6.  **Debugging Analysis**: The `debugger` agent analyzes all the diagnostics from the previous steps (syntax errors, runtime output, pytest results) and provides a summary of issues. If a fix is possible, it generates a patch.
7.  **Patch Application**: If a patch is generated, the workflow can either apply it automatically or prompt the user for approval before starting the next iteration.

## Project Structure

```
/
├── agents/
│   ├── researcher.py
│   ├── coder.py
│   ├── test_writer.py
│   └── debugger.py
├── utils/
│   ├── inspector.py
│   └── test_runner.py
├── workflows/
│   └── ml_coach.py
├── generated_code.py       # (Output) The code generated by the workflow
└── test_generated.py       # (Output) The tests generated by the workflow
```

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <your-repository-url>
    cd <repository-name>
    ```
2.  **Install dependencies:** The primary dependency for running tests is `pytest`.
    ```bash
    pip install pytest
    ```
3.  **Set up API Key:** The workflow requires a Google API key. Set it as an environment variable or directly in `workflows/ml_coach.py`.
    ```python
    # In workflows/ml_coach.py
    os.environ["GOOGLE_API_KEY"] = "your-api-key-here"
    ```

## Running the Workflow

The main entry point is the `ml_coach.py` script. You can run it directly from your terminal.

```bash
python -m workflows.ml_coach
```

The default prompt in the script asks the model to explain transformers and write a simple token-counting function. You can modify the `prompt` variable in the `if __name__ == "__main__"` block to suit your needs.

## Configuration

You can configure the workflow's behavior using environment variables:

- **`MAX_ITERS`**: The maximum number of debugging loops to run. Defaults to `3`.
  ```bash
  export MAX_ITERS=5
  ```
- **`RUN_TIMEOUT`**: The timeout in seconds for executing the generated code. Defaults to `8`.
  ```bash
  export RUN_TIMEOUT=10
  ```
- **`AUTO_PATCH`**: Set to `1` to have the workflow automatically apply all suggested patches without prompting the user.
  ```bash
  export AUTO_PATCH=1
  ```
